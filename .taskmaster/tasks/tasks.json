{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Scaffolding and Environment Setup",
        "description": "Set up the initial project structure with Python 3.11+, development tools, and directory layout according to the PRD specifications.",
        "details": "1. Create a new Python project with Python 3.11+ support\n2. Set up development tools: uv, ruff, black, mypy, pytest\n3. Create directory structure following the Clean/Hex architecture:\n   - domain/\n   - application/\n   - adapters/\n   - ports/\n   - cli/\n4. Initialize pyproject.toml with entry points: `sloptest = smart_test_generator.cli.main:app`\n5. Set up .gitignore, README.md, and LICENSE files\n6. Configure development environment with virtual environment\n7. Create initial package structure with __init__.py files",
        "testStrategy": "Verify project structure exists with correct directories. Ensure all development tools can be invoked. Validate pyproject.toml configuration with a simple import test.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Python Project and Virtual Environment",
            "description": "Create a new Python project directory, ensure Python 3.11+ is available, and set up a virtual environment for isolated development.",
            "dependencies": [],
            "details": "Establish the root project folder, verify Python 3.11+ installation, and create a virtual environment using the preferred tool (e.g., venv, uv, or Poetry). Activate the environment for subsequent steps.",
            "status": "done",
            "testStrategy": "Check that the virtual environment is active and Python version is 3.11 or higher by running 'python --version' and verifying isolation from global packages."
          },
          {
            "id": 2,
            "title": "Configure Development Tools and Linting",
            "description": "Install and configure development tools: uv (or Poetry/PDM), ruff, black, mypy, and pytest for code formatting, linting, type checking, and testing.",
            "dependencies": [
              "1.1"
            ],
            "details": "Use the package manager to install ruff, black, mypy, and pytest as development dependencies. Create or update configuration files for each tool (e.g., pyproject.toml sections or standalone config files) to enforce code quality standards.\n<info added on 2025-09-06T21:22:00.312Z>\nProject name has been updated to \"testcraft\" from \"smart-test-generator\". The pyproject.toml file has been modified to reflect this change, including updating the project name and adjusting the pytest coverage path to match the new structure. All development tools (ruff, black, mypy, pytest) have been successfully configured with appropriate settings and verified to be working correctly.\n</info added on 2025-09-06T21:22:00.312Z>",
            "status": "done",
            "testStrategy": "Run each tool (ruff, black, mypy, pytest) on a sample file to confirm correct installation and configuration."
          },
          {
            "id": 3,
            "title": "Establish Project Directory Structure (Clean/Hex Architecture)",
            "description": "Create the core directory layout: domain/, application/, adapters/, ports/, cli/, and ensure each contains an __init__.py file for package recognition.",
            "dependencies": [
              "1.1"
            ],
            "details": "Manually or via script, generate the specified directories and add empty __init__.py files to each. Follow Clean/Hex architecture conventions for separation of concerns.",
            "status": "done",
            "testStrategy": "Verify the presence of all required directories and __init__.py files. Attempt to import each package in a Python shell to confirm discoverability."
          },
          {
            "id": 4,
            "title": "Initialize pyproject.toml and Project Metadata",
            "description": "Create and configure pyproject.toml with project metadata, dependencies, tool configurations, and entry points as specified.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Define project name, version, authors, and dependencies in pyproject.toml. Add entry point: 'sloptest = smart_test_generator.cli.main:app'. Include tool configuration sections for ruff, black, mypy, and pytest as needed.",
            "status": "done",
            "testStrategy": "Validate pyproject.toml syntax and confirm entry point is discoverable by running 'python -m smart_test_generator.cli.main' or equivalent."
          },
          {
            "id": 5,
            "title": "Add Essential Project Files and Version Control",
            "description": "Create .gitignore, README.md, and LICENSE files. Initialize a Git repository and make the initial commit.",
            "dependencies": [
              "1.1",
              "1.3",
              "1.4"
            ],
            "details": "Draft a .gitignore tailored for Python projects, write a basic README.md with project overview and setup instructions, and select an appropriate open-source LICENSE. Initialize Git and commit all scaffolding files.",
            "status": "done",
            "testStrategy": "Verify that .gitignore excludes virtual environment and build artifacts, README.md renders correctly on GitHub, LICENSE is present, and 'git status' shows a clean working directory after commit."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Domain Models",
        "description": "Create the core domain models using pydantic to represent the fundamental entities in the system.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "1. Create domain/models.py with the following models:\n   - TestGenerationPlan (elements_to_test, existing_tests, coverage_before)\n   - TestElement (name, type, line_range, docstring)\n   - CoverageResult (line_coverage, branch_coverage, missing_lines)\n   - GenerationResult (file_path, content, success, error_message)\n   - RefineOutcome (updated_files, rationale, plan)\n   - AnalysisReport (files_to_process, reasons, existing_test_presence)\n2. Use pydantic for validation and serialization\n3. Implement proper type hints for all models\n4. Add docstrings explaining each model's purpose and fields\n5. Ensure models are immutable where appropriate\n6. Implement TestElementType enum for categorizing test elements (function, class, method, module)\n7. Add custom validators for data integrity:\n   - Line ranges must be valid (start <= end, positive numbers)\n   - Coverage percentages must be between 0.0 and 1.0\n   - Missing lines are sorted and deduplicated\n   - Error messages required when generation fails\n   - All required mappings must cover all files in lists",
        "testStrategy": "Unit tests for each model verifying initialization, validation rules, serialization/deserialization, and edge cases like empty values or invalid inputs. Specifically test custom validators for line ranges, coverage percentages, and other data integrity rules.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement TestElement model",
            "description": "Create TestElement model with name, type, line range, and docstring fields",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CoverageResult model",
            "description": "Create CoverageResult model with line/branch coverage and missing lines",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement GenerationResult model",
            "description": "Create GenerationResult model with file path, content, success/failure status and error message",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement TestGenerationPlan model",
            "description": "Create TestGenerationPlan model with elements to test, existing tests, and coverage information",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement RefineOutcome model",
            "description": "Create RefineOutcome model with updated files, rationale and plan",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement AnalysisReport model",
            "description": "Create AnalysisReport model with files to process, reasons, and existing test presence",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement TestElementType enum",
            "description": "Create enum for categorizing test elements (function, class, method, module)",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add custom validators",
            "description": "Implement validators for line ranges, coverage percentages, missing lines, and other data integrity rules",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Configure model immutability",
            "description": "Set models as immutable using `frozen = True` configuration",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Add field descriptions",
            "description": "Add detailed field descriptions for better API documentation",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Configuration System",
        "description": "Create a typed configuration system using pydantic that validates on load, provides sensible defaults, and supports all required configuration segments.",
        "details": "1. Create config/model.py with pydantic models for each configuration segment:\n   - TestGenerationConfig (minimum_line_coverage, etc.)\n   - CoverageConfig (minimum_line_coverage)\n   - MergeConfig (strategy: Literal[\"append\", \"ast-merge\"])\n   - TestRunnerConfig (enable: bool)\n   - RefineConfig (enable: bool, max_retries, backoff, caps)\n   - ContextConfig (retrieval settings, hybrid weights, rerank model, hyde: bool)\n   - SecurityConfig (block_patterns: list[str], max_generated_file_size)\n   - CostConfig (daily_limit, per_request_limit)\n   - EnvironmentConfig\n   - QualityConfig\n   - PromptEngineeringConfig\n2. Create config/loader.py to merge YAML+env+CLI sources\n3. Implement validation logic for all configuration parameters\n4. Add sensible defaults for all non-required fields\n5. Support environment variable overrides with prefix\n6. Add helper methods for accessing nested configuration",
        "testStrategy": "Unit tests verifying configuration loading from different sources, validation of required fields, default values, environment variable overrides, and error handling for invalid configurations.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Define Interface Ports",
        "description": "All 12 interface ports have been implemented using Python Protocols, providing clear contracts between the application layer and adapters. Each port is defined in its own file under testcraft/ports/, with comprehensive docstrings, precise type hints, and no adapter imports. The __init__.py file exports all port interfaces. These protocols are now ready for adapter implementations.",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "details": "1. 12 protocol interface files created in testcraft/ports/:\n   - LLMPort: Large Language Model operations (test generation, analysis, refinement)\n   - CoveragePort: Code coverage measurement and reporting\n   - WriterPort: File writing operations (test files, reports)\n   - ParserPort: Source code parsing and AST analysis\n   - ContextPort: Context management (indexing, retrieval, summarization)\n   - PromptPort: Prompt management (system/user prompts, schemas)\n   - RefinePort: Test refinement operations\n   - StatePort: Application state management\n   - ReportPort: Report generation (analysis, coverage, summaries)\n   - UIPort: User interface operations (progress, results, input)\n   - CostPort: Cost tracking and usage monitoring\n   - TelemetryPort: Observability and metrics collection\n2. All protocols use Python's typing.Protocol for interface definitions\n3. Comprehensive docstrings provided for each method\n4. All parameters and return values have precise type hints\n5. No imports from adapters in any port file\n6. __init__.py updated to export all port interfaces\n7. Consistent naming and structure across all protocols\n8. Custom exception types used for error handling\n9. Rich return types with detailed metadata\n10. Support for configuration options via **kwargs\n11. Integration with domain models (CoverageResult, RefineOutcome, AnalysisReport)\n12. No linting errors detected",
        "testStrategy": "Test implementations for each protocol verify interface compliance. mypy is used to ensure type checking works correctly with all protocols. Linting and static analysis confirm code quality and adherence to standards.",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify protocol files and exports",
            "description": "Check that all 12 protocol files exist in testcraft/ports/ and that __init__.py exports each interface.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Run mypy and linting on ports directory",
            "description": "Ensure all protocol files pass mypy type checking and linting with no errors.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create test implementations for each protocol",
            "description": "Write minimal test classes for each protocol to verify interface compliance and demonstrate usage.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Parser Adapters",
        "description": "Create adapters for parsing Python code and mapping tests to source code elements. The parser must extract both structural metadata and the actual source code content for each code element to support test generation.",
        "status": "done",
        "dependencies": [
          4
        ],
        "priority": "high",
        "details": "1. Create adapters/parsing/codebase_parser.py:\n   - Implement AST-based parsing of Python files\n   - Extract functions, classes, methods with signatures\n   - Parse docstrings and type annotations\n   - Identify import statements and dependencies\n   - Extract the raw source code content for each code element (function, class, method) in addition to metadata\n   - Ensure parsing results include both metadata (name, type, line_range, docstring) and the actual implementation code\n   - Consider whether the TestElement domain model should be extended with a 'source_code' field, or if this content should be handled separately in the parsing pipeline\n2. Create adapters/parsing/test_mapper.py:\n   - Map test functions to source code elements\n   - Identify existing test coverage for elements\n   - Support pytest naming conventions\n   - Ensure mapping logic can access both metadata and source code content for each element\n3. Implement helper functions for:\n   - Building directory trees with bounded depth/width\n   - Extracting element signatures and source code\n   - Identifying uncovered elements\n   - Caching parsed files to improve performance, including both metadata and source code content",
        "testStrategy": "Unit tests with sample Python files to verify correct parsing of functions, classes, methods, and imports. Test that both metadata and actual source code content are extracted for each code element. Test mapping of test functions to source elements with various naming conventions, ensuring access to source code content. Verify directory tree generation with different depth/width constraints.",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Coverage Adapters",
        "description": "All coverage adapters for measuring code coverage using pytest+coverage with AST fallback are now implemented and production-ready. The solution provides robust coverage measurement, fallback capabilities, and comprehensive reporting.\n\nKey components:\n- adapters/coverage/pytest_coverage.py: Full pytest+coverage integration with command building, environment setup, coverage parsing (JSON/XML), timeout handling, and artifact storage in .testcraft/coverage/<run_id>\n- adapters/coverage/ast_fallback.py: AST-based coverage estimation with executable line detection, complex construct analysis, private function filtering, and realistic coverage estimation\n- adapters/coverage/composite.py: Composite adapter that tries pytest first and falls back to AST with reason tracking and unified interface\n- adapters/coverage/main_adapter.py: Primary adapter implementing CoveragePort interface with all required methods, HTML/JSON report generation, and comprehensive coverage analysis\n- adapters/coverage/__init__.py: Exports all adapters\n- test_coverage_adapters.py: Comprehensive test suite with unit and integration tests for all adapters\n\nAll adapters handle error cases, timeouts, and edge cases. Code is lint-free and follows project standards.",
        "status": "done",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "details": "Coverage adapters are implemented as follows:\n\n1. adapters/coverage/pytest_coverage.py:\n   - Builds and runs pytest+coverage commands with correct cwd, env, and pythonpath\n   - Parses coverage results from JSON and XML outputs\n   - Handles timeouts, subprocess errors, and stores artifacts in .testcraft/coverage/<run_id>\n2. adapters/coverage/ast_fallback.py:\n   - Estimates line and branch coverage using AST analysis\n   - Detects executable lines, analyzes complex constructs, filters private functions\n   - Provides realistic fallback coverage estimation when pytest coverage fails\n3. adapters/coverage/composite.py:\n   - Attempts pytest coverage first, falls back to AST estimation on failure\n   - Records reason for fallback and exposes a unified interface\n4. adapters/coverage/main_adapter.py:\n   - Implements CoveragePort interface\n   - Provides HTML and JSON report generation\n   - Aggregates and analyzes coverage results from all adapters\n5. __init__.py exports all adapters for external use\n6. Comprehensive test suite (test_coverage_adapters.py):\n   - Unit tests for each adapter\n   - Integration tests for composite behavior and error handling\n   - Tests for edge cases, timeouts, and artifact correctness\n\nAll code is linted and adheres to project standards.",
        "testStrategy": "Comprehensive test suite with unit tests for all adapters and integration tests for composite behavior. Tests verify coverage measurement with pytest, AST fallback with various Python constructs, correct fallback logic, error and timeout handling, and artifact storage. All adapters are validated against edge cases and project linting standards.",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Writer Adapters",
        "description": "Create adapters for safely writing generated tests to the filesystem with different strategies. All writes must be formatted with Black and isort for consistent code formatting.",
        "status": "done",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "details": "1. Create adapters/io/writer_append.py:\n   - Implement simple append to existing test files\n   - Create new files when missing\n   - Support dry-run mode\n   - Format all writes with Black and isort\n2. Create adapters/io/writer_ast_merge.py:\n   - Parse existing and new test files\n   - Merge structurally to avoid duplicates\n   - Format all writes with Black and isort\n   - Generate unified diff for dry-run\n3. Implement safety policies:\n   - Only write to tests/ directory\n   - Enforce file size caps\n   - Block dangerous patterns\n   - Validate syntax before writing\n4. Add helper functions for path resolution and validation\n5. Integrate Black and isort formatting into all write operations, including dry-run output.",
        "testStrategy": "Unit tests for append and AST merge strategies with various test files. Verify safety policies block writes outside tests/ directory and files with dangerous patterns. Test dry-run mode generates correct diffs without modifying files. Validate that all written and diffed files are formatted with Black and isort.",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Black and isort formatting into writer_append.py",
            "description": "Update writer_append.py so that all writes (including dry-run output) are formatted using Black and isort before being written or diffed.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:03:58.368Z>\n✅ COMPLETED - Black and isort formatting integration in writer_append.py\n\nImplementation Details:\n- Added _format_content() method that uses subprocess to call both isort and Black\n- Format pipeline: content → isort (import sorting) → Black (code formatting) → formatted output\n- Graceful fallback: returns original content if formatting tools aren't available\n- All write operations automatically format content before writing or showing diffs\n- Dry-run mode also shows formatted content in previews\n\nKey Features:\n- Temporary file approach for safe formatting without affecting original content\n- Error handling prevents failures when Black/isort not installed\n- Consistent formatting applied to both new files and appended content\n- Integration verified through unit tests with subprocess mocking\n</info added on 2025-09-07T15:03:58.368Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Black and isort formatting into writer_ast_merge.py",
            "description": "Update writer_ast_merge.py so that all merged output (including dry-run diff) is formatted using Black and isort before being written or diffed.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:04:17.585Z>\n✅ COMPLETED - Black and isort formatting integration in writer_ast_merge.py\n\nImplementation Details:\n- Added a _format_content() method, mirroring the approach used in writer_append.py, to ensure consistent formatting logic across writer adapters.\n- The formatting pipeline applies isort for import sorting followed by Black for code formatting to all merged content before output, guaranteeing standardized and readable code.\n- Both the final merged output and the dry-run diff generation now utilize this formatting pipeline, ensuring that diffs accurately reflect the formatted result.\n- The implementation includes graceful fallback handling: if Black or isort are unavailable, the merge proceeds without formatting rather than failing.\n\nAST Merge Features Enhanced:\n- The AST merging logic now preserves the existing code structure while intelligently adding new elements, minimizing unnecessary changes.\n- Enhanced deduplication ensures that imports, functions, and classes are merged without introducing duplicates.\n- The merge process is structurally aware: imports are placed first, followed by constants, classes, functions, and then other statements, resulting in a logical and maintainable code order.\n- If AST parsing fails, the system falls back to simple concatenation to avoid blocking the merge process.\n- Dry-run mode now generates a unified diff that precisely shows the changes that would be made after formatting, improving transparency and reviewability.\n</info added on 2025-09-07T15:04:17.585Z>",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update unit tests to validate Black and isort formatting",
            "description": "Extend unit tests to check that all written and diffed files are correctly formatted with Black and isort.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:04:22.973Z>\nComprehensive unit tests have been implemented to validate all core writer adapter behaviors, including strict enforcement of Black and isort formatting on written and diffed files. Tests cover SafetyPolicies (path validation, size limits, dangerous pattern detection, Python syntax validation, test file naming), WriterAppendAdapter (file creation, append, dry-run, backup, directory creation, formatting integration), and WriterASTMergeAdapter (AST merge logic, deduplication, diff generation, merge fallbacks). Key features include mocked subprocess calls for Black/isort, fallback behavior when formatting fails, exhaustive safety policy scenarios, dry-run mode validation, complex AST merge cases, and robust error/exception handling.\n</info added on 2025-09-07T15:04:22.973Z>",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement State Management",
        "description": "Create adapters for managing state across runs, including coverage history and generation logs, using a clean JSON state storage system.",
        "status": "done",
        "dependencies": [
          4
        ],
        "priority": "medium",
        "details": "1. Create adapters/io/state_json.py:\n   - Implement project-scoped JSON state storage\n   - Maintain coverage history\n   - Track generation log\n   - Support idempotent decisions\n2. Implement methods for:\n   - Initializing state\n   - Updating state after generation\n   - Querying historical data\n   - Determining which files need generation\n3. Implement state synchronization and reset commands",
        "testStrategy": "Unit tests for state initialization, updates, and queries. Verify idempotent decisions work correctly across multiple runs. Test state reset and synchronization.",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Telemetry and Cost Management",
        "description": "Production-ready modular telemetry and cost management system with privacy-first features, multi-backend support, and comprehensive metrics/cost tracking.",
        "status": "done",
        "dependencies": [
          4
        ],
        "priority": "medium",
        "details": "Implementation now includes:\n\n1. Modular Telemetry Port Interface (testcraft/ports/telemetry_port.py):\n   - Defines a unified interface for telemetry operations (spans, metrics, privacy controls)\n   - Supports multiple backends (OpenTelemetry, Datadog, Jaeger, No-Op)\n   - Enables easy backend swapping and extension\n\n2. OpenTelemetry Adapter (testcraft/adapters/telemetry/opentelemetry_adapter.py):\n   - Full OpenTelemetry integration with automatic span/context management\n   - Graceful fallback to console or no-op if OpenTelemetry is unavailable\n   - Built-in anonymization and opt-out support\n   - Metrics collection: counters, histograms, gauges for LLM calls, coverage, file ops, test generation\n   - OTLP export and privacy protection\n\n3. No-Op Adapter (testcraft/adapters/telemetry/noop_adapter.py):\n   - Implements the telemetry interface but performs no operations\n   - Used for disabled telemetry/testing scenarios\n   - Zero overhead when telemetry is off\n\n4. Cost Management System (testcraft/adapters/telemetry/cost_manager.py):\n   - Implements CostPort interface for tracking token usage and costs\n   - Budget enforcement with configurable limits/warnings\n   - Persistent cost data storage (JSON export)\n   - Daily/weekly/monthly summaries and breakdowns\n   - Integrated with telemetry for cost-related metrics\n\n5. Telemetry Router/Factory (testcraft/adapters/telemetry/router.py):\n   - Factory pattern for adapter instantiation based on config\n   - Registry for custom adapter registration\n   - Context managers for telemetry operations\n   - Automatic fallback to no-op adapter\n\n6. Configuration Integration (testcraft/config/models.py):\n   - TelemetryConfig and TelemetryBackendConfig classes\n   - Supports backend selection, privacy/anonymization, sampling rates, cost thresholds\n   - Validates backend-specific options\n\n7. Comprehensive Test Suite (tests/test_telemetry_adapters.py):\n   - Unit tests for all adapters and interfaces\n   - Integration tests for cost management and telemetry\n   - Error handling and edge case coverage\n   - Mock-based testing for external dependencies\n\nKey Features:\n- Modular design for backend flexibility\n- Privacy-first: anonymization and opt-out\n- Cost control: budget enforcement, optimization, persistent storage\n- Graceful degradation: works without OpenTelemetry\n- Comprehensive metrics: LLM calls, coverage, file ops, test generation\n- Export capabilities: CSV/JSON for cost analysis\n\nUsage Example:\n\nfrom testcraft.adapters.telemetry import create_telemetry_adapter, CostManager\n\ntelemetry = create_telemetry_adapter(config.telemetry)\ncost_manager = CostManager(config.cost_management, telemetry)\n\nwith telemetry.create_span(\"llm_call\") as span:\n    span.set_attribute(\"model\", \"gpt-4\")\n    # ... perform LLM operation\n    cost_manager.track_usage(\"llm\", \"generate_tests\", {\"cost\": 0.50, \"tokens_used\": 200})",
        "testStrategy": "Comprehensive unit and integration tests for all telemetry adapters and cost management components. Tests cover span creation, attribute recording, metrics collection, aggregation, cost tracking, budget enforcement, opt-out, anonymization, error handling, and external dependency mocking. Persistent storage and export functionality are verified.",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Reporting and Artifact Storage",
        "description": "Create adapters for generating reports and storing artifacts from test generation runs.",
        "details": "1. Create adapters/io/reporter_json.py:\n   - Generate structured JSON reports\n   - Include coverage delta, tests generated, pass rate\n   - Record prompts and schemas (when verbose)\n   - Summarize retrieval diagnostics\n2. Create adapters/io/artifact_store.py:\n   - Store coverage reports\n   - Save generated tests\n   - Preserve LLM responses\n   - Manage run history\n   - Implement cleanup policies\n3. Implement helper functions for:\n   - Formatting tables for CLI output\n   - Generating spinners and progress indicators\n   - Creating concise summaries",
        "testStrategy": "Unit tests for report generation with various inputs. Test artifact storage and retrieval. Verify cleanup policies work correctly. Test formatting functions for CLI output.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design JSON Report Schema and Adapter",
            "description": "Define the structure for JSON reports and implement the reporter_json.py adapter to generate structured reports including coverage delta, tests generated, pass rate, prompts, schemas, and retrieval diagnostics.",
            "dependencies": [],
            "details": "Establish a clear schema for all required report fields. Implement logic to serialize test run data into this schema, supporting both standard and verbose modes.",
            "status": "done",
            "testStrategy": "Unit tests for report generation with various input scenarios, including edge cases and verbose output."
          },
          {
            "id": 2,
            "title": "Implement Artifact Storage Adapter",
            "description": "Develop artifact_store.py to store coverage reports, generated tests, LLM responses, and manage run history with cleanup policies.",
            "dependencies": [],
            "details": "Create methods for saving, retrieving, and cleaning up artifacts. Ensure compatibility with different artifact types and implement configurable cleanup strategies.",
            "status": "done",
            "testStrategy": "Test artifact storage and retrieval for all supported types. Verify cleanup policies remove artifacts as expected."
          },
          {
            "id": 3,
            "title": "Develop Rich-based Table Formatting Helpers",
            "description": "Create helper functions using Rich to format tables for CLI output, ensuring clear and visually appealing presentation of report data.",
            "dependencies": [],
            "details": "Utilize Rich's Table and Console APIs to render tabular data such as test results, coverage summaries, and diagnostics in the CLI.",
            "status": "done",
            "testStrategy": "Unit tests for table formatting with various data sets. Visual inspection for alignment, color, and readability."
          },
          {
            "id": 4,
            "title": "Implement Spinners and Progress Indicators with Rich",
            "description": "Develop CLI helpers for spinners and progress bars using Rich to provide real-time feedback during long-running operations.",
            "dependencies": [],
            "details": "Leverage Rich's Progress and Spinner components to indicate activity during report generation, artifact storage, and test runs.",
            "status": "done",
            "testStrategy": "Test spinner and progress bar display during simulated long-running tasks. Verify correct updates and completion states."
          },
          {
            "id": 5,
            "title": "Create Concise Summary Generation Helpers",
            "description": "Implement functions to generate concise, human-readable summaries of test runs and diagnostics for CLI output.",
            "dependencies": [],
            "details": "Summarize key metrics and outcomes using Rich panels or layouts for quick user comprehension.",
            "status": "done",
            "testStrategy": "Test summary generation with diverse input data. Validate clarity and completeness of summaries."
          },
          {
            "id": 6,
            "title": "Integrate Theming and Layouts for Professional CLI",
            "description": "Apply Rich theming, panels, and layouts to ensure a visually professional and consistent CLI interface.",
            "dependencies": [
              "10.3",
              "10.4",
              "10.5"
            ],
            "details": "Define a color palette and layout strategy. Use Rich's theming and layout features to unify the appearance of all CLI outputs.",
            "status": "done",
            "testStrategy": "Visual inspection and user feedback for theme consistency and layout effectiveness."
          },
          {
            "id": 7,
            "title": "Implement UIPort Integration for CLI Output",
            "description": "Integrate all Rich-based UI components with the UIPort abstraction to standardize CLI output routing and enable future extensibility.",
            "dependencies": [
              "10.6"
            ],
            "details": "Ensure all output (tables, spinners, summaries) is routed through UIPort, supporting both interactive and non-interactive modes.",
            "status": "done",
            "testStrategy": "Test UIPort output in different CLI contexts. Verify correct rendering and fallback behavior."
          },
          {
            "id": 8,
            "title": "Document and Test the Complete Reporting and Storage System",
            "description": "Write comprehensive documentation and end-to-end tests for the reporting and artifact storage adapters, including UI helpers and integration points.",
            "dependencies": [
              "10.1",
              "10.2",
              "10.3",
              "10.4",
              "10.5",
              "10.6",
              "10.7"
            ],
            "details": "Document usage, configuration, and extension points. Develop integration tests covering typical and edge-case workflows.",
            "status": "done",
            "testStrategy": "Review documentation for completeness. Run integration tests simulating real user workflows and verify expected outcomes."
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Prompt Registry and Templates",
        "description": "Create a registry for versioned prompt templates with system and user prompts for generation and refinement.",
        "details": "1. Create prompts/registry.py:\n   - Implement versioned templates\n   - Store system prompts for generation and refinement\n   - Store user prompt templates\n   - Define JSON schemas for structured outputs\n2. Create generation system prompt with:\n   - Role: \"Python Test Generation Agent\"\n   - Constraints on output format and safety\n   - Guardrails against modifying source files\n3. Create refinement system prompt with:\n   - Role: \"Python Test Refiner\"\n   - Constraints on fixing specific issues\n4. Implement JSON schemas for:\n   - Generation output (file path and content)\n   - Refinement output (updated files, rationale, plan)\n5. Add anti-injection defenses in prompts",
        "testStrategy": "Unit tests for prompt template rendering with various inputs. Verify JSON schemas validate correct outputs and reject invalid ones. Test anti-injection defenses with potentially problematic inputs.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Context Pipeline",
        "description": "Create adapters for indexing, retrieving, and summarizing code context from the repository.",
        "details": "1. Create context/indexer.py:\n   - Implement hybrid index (BM25 + dense)\n   - Store chunk metadata (path, symbol, imports)\n   - Support adaptive chunking (semantic boundaries, AST nodes)\n   - Typical chunk size 300-800 tokens\n2. Create context/retriever.py:\n   - Build queries from filename, symbols, docstrings\n   - Include uncovered lines context\n   - Support optional recent git diffs\n   - Implement HyDE expansion and query rewriting\n   - Rerank with cross-encoder\n   - Select top-k snippets\n3. Create context/summarizer.py:\n   - Generate directory tree with bounded breadth/depth\n   - Summarize imports and class signatures\n   - Enforce max chars for user prompt content\n4. Implement context budgeting and truncation strategies",
        "testStrategy": "Unit tests for indexing with sample repositories. Test retrieval with various queries and verify relevant snippets are returned. Test summarization with different directory structures. Verify context budgeting works correctly with large repositories.",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement LLM Adapters",
        "description": "Create adapters for different LLM providers with common helpers for response parsing, validation, and error handling.",
        "details": "1. Create llm/common.py:\n   - Implement response parsing with brace balancing\n   - Add strict JSON Schema validation\n   - Support repair attempts for minor issues\n   - Normalize output (strip code fences, unescape)\n   - Log cost and token usage\n   - Implement retries with jitter\n   - Respect rate limits\n   - Set timeouts\n2. Create provider-specific adapters:\n   - llm/claude.py for Anthropic Claude\n   - llm/openai.py for OpenAI models\n   - llm/azure.py for Azure OpenAI\n   - llm/bedrock.py for AWS Bedrock\n3. Implement model routing based on file complexity\n4. Use deterministic settings for structure (temperature 0.2-0.3 for generation; lower for refine)\n5. Support streaming responses for large outputs",
        "testStrategy": "Unit tests for each provider adapter with mocked responses. Test response parsing with various formats including malformed JSON. Verify retry logic works with different error types. Test model routing with files of varying complexity.",
        "priority": "high",
        "dependencies": [
          4,
          11
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Refinement System",
        "description": "Create a single adapter for refining generated tests based on pytest failures and other issues, leveraging LLM analysis of failure output and code context.",
        "status": "done",
        "dependencies": [
          6,
          7,
          13
        ],
        "priority": "medium",
        "details": "1. Create refine/adapter.py:\n   - Accept pytest failure output and current test code\n   - Send both, along with relevant codebase context, to the LLM\n   - Receive refined test code from the LLM\n   - Safely apply changes to the test file\n   - Rerun pytest to verify fixes\n2. Implement iteration management:\n   - Cap the number of refinement attempts (e.g., max 3)\n   - Detect and stop on no-change between iterations\n   - Enforce time limits for safety\n   - Gracefully degrade if refinement fails (e.g., log and exit)\n3. Build payloads for LLM:\n   - Include failure output (stdout/stderr)\n   - Provide current test file content\n   - Add relevant source code context\n   - Track previous refinement attempts to avoid infinite loops\n\nThis approach eliminates explicit failure categorization and separate strategy classes, relying on the LLM's ability to analyze and fix diverse test failures directly.",
        "testStrategy": "Unit tests for adapter with sample pytest outputs and test files. Verify that the adapter sends correct payloads to the LLM and applies changes safely. Test iteration caps, no-change detection, and time limits. Simulate refinement failures and verify graceful handling.",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement Analyze Use Case",
        "description": "Create the application layer use case for analyzing what would be generated and why.",
        "details": "1. Create application/analyze_usecase.py:\n   - Implement steps: sync state → find files → coverage.measure\n   - For each file call state.should_generate\n   - Build plans (elements from parser; reasons; existing test presence)\n   - Produce AnalysisReport DTO\n2. Use dependency injection for ports\n3. Keep business logic pure (no direct adapter calls)\n4. Implement helper functions for:\n   - Determining which elements need tests\n   - Calculating coverage gaps\n   - Prioritizing files for generation",
        "testStrategy": "Unit tests with mocked ports to verify correct analysis flow. Test with various repository states and coverage levels. Verify correct identification of elements needing tests and reasons for generation.",
        "priority": "medium",
        "dependencies": [
          4,
          5,
          6,
          8
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Implement Coverage Use Case",
        "description": "Create the application layer use case for measuring and reporting code coverage.",
        "details": "1. Create application/coverage_usecase.py:\n   - Implement steps: sync state → find files → coverage.measure → coverage.report\n   - Support filtering by file patterns\n   - Calculate overall and per-file coverage\n   - Generate coverage reports\n2. Use dependency injection for ports\n3. Keep business logic pure (no direct adapter calls)\n4. Implement helper functions for:\n   - Formatting coverage results\n   - Identifying coverage trends over time\n   - Suggesting files for improvement",
        "testStrategy": "Unit tests with mocked ports to verify correct coverage measurement and reporting. Test with various repository states and coverage levels. Verify correct calculation of overall and per-file coverage.",
        "priority": "medium",
        "dependencies": [
          4,
          6,
          8
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement Generate Use Case",
        "description": "The core Generate Use Case is now fully implemented as a production-ready application layer orchestrator for test generation. It coordinates the end-to-end workflow, leveraging all finalized LLM adapters, refinement, coverage, context, and writer systems. The implementation strictly follows clean architecture principles, with pure business logic separated from adapters and robust dependency injection for all ports. The workflow is asynchronous, supports batching and concurrency, and includes comprehensive telemetry, error handling, and resource management. All integration points are validated with real adapters, and the system gracefully degrades on non-critical failures. The use case is highly configurable and supports iterative refinement, context gathering, and detailed reporting.",
        "status": "done",
        "dependencies": [
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
        ],
        "priority": "high",
        "details": "Implementation highlights:\n- Full class structure with dependency injection for all required ports (LLM, Writer, Coverage, Refine, Context, Parser, State, Telemetry)\n- Async/await pattern throughout for optimal concurrency\n- State synchronization and file discovery logic\n- File processing decision logic and TestGenerationPlan creation\n- Directory tree building and context retrieval (configurable)\n- Batching and concurrency strategies for LLM calls and writing\n- LLM test generation with prompt building, validation, and normalization\n- Test file writing with configured strategies\n- Pytest execution and iterative refinement logic with capped attempts and no-change detection\n- Coverage delta measurement and comprehensive reporting\n- Telemetry and metrics recording with detailed spans and attributes\n- Robust error handling and graceful degradation on non-critical failures (context, coverage, etc.)\n- Resource cleanup and management\n- Pure business logic in orchestration, no direct adapter calls\n- Fully integrated with real LLM adapters and refinement system\n- Configurable batching, refinement, and context gathering\n- Thread pool for concurrent operations\n- All blocking notes and references to incomplete dependencies removed\n\nWorkflow steps:\n1. Sync state & discover files\n2. Measure initial coverage\n3. Decide files to process\n4. Build generation plans\n5. Gather project context\n6. Execute test generation (batched)\n7. Write test files\n8. Execute & refine tests\n9. Measure final coverage\n10. Record state & telemetry",
        "testStrategy": "Unit tests with mocked ports to verify correct generation flow, including async and concurrency scenarios. Test with various repository states, coverage levels, and configuration options. Verify correct handling of LLM responses, writing strategies, and iterative refinement. Test error handling, graceful degradation, and resource cleanup. Integration tests with real LLM and refinement adapters to ensure end-to-end functionality, correct adapter wiring, and telemetry reporting.",
        "subtasks": [
          {
            "id": 1,
            "title": "Document architecture and workflow",
            "description": "Write comprehensive documentation for the Generate Use Case, covering class structure, dependency injection, async workflow, configuration options, and integration points. Include diagrams and code samples illustrating the orchestration logic and adapter separation.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Expand integration test coverage",
            "description": "Add additional integration tests targeting edge cases in batching, concurrency, error handling, and graceful degradation. Ensure tests cover real adapter interactions and telemetry reporting.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 18,
        "title": "Implement Status and Utility Use Cases",
        "description": "Create application layer use cases for viewing generation state/history and utility functions. StatusUseCase is implemented with comprehensive status, filtering, sorting, statistics, and proper error handling. Next, implement UtilityUseCases for debug-state, sync-state, reset-state, env, and cost utilities following the same patterns.",
        "status": "done",
        "dependencies": [
          4,
          8,
          9
        ],
        "priority": "low",
        "details": "1. application/status_usecase.py:\n   - StatusUseCase implemented using established patterns (constructor, exception handling, telemetry, logging).\n   - get_generation_status(): Returns comprehensive status including current state, history, statistics, and file-level status.\n   - get_filtered_history(): Supports filtering/sorting by date, status, and other options.\n   - Integrates with StatePort, TelemetryPort, FileDiscoveryService via dependency injection.\n   - Uses StatusUseCaseError for error handling and telemetry span tracking.\n   - Configurable defaults (max_history_entries, time windows, etc.).\n2. application/utility_usecases.py:\n   - Implement utility use cases: debug-state (dump internal state), sync-state (force state sync), reset-state (clear state), env (show environment info), cost (display cost summary/projections).\n   - Follow same patterns as StatusUseCase: dependency injection, error handling, telemetry, logging, and configuration.\n3. All business logic remains pure (no direct adapter calls).",
        "testStrategy": "Unit tests with mocked ports to verify correct status reporting and utility functions. Test StatusUseCase with various repository states, history records, and configuration options. For UtilityUseCases, test each utility (debug-state, sync-state, reset-state, env, cost) with different internal states and error scenarios. Verify correct handling of state operations, environment info, and cost projections. Ensure all use cases are isolated and testable via dependency injection.",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze existing use case patterns and identify abstractions",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement StatusUseCase in application/status_usecase.py",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement UtilityUseCases in application/utility_usecases.py for debug-state, sync-state, reset-state, env, and cost utilities following established patterns",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Write unit tests for StatusUseCase covering status, filtering, sorting, statistics, and error handling",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Write unit tests for UtilityUseCases covering all utility functions and error scenarios",
            "description": "",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 19,
        "title": "Implement CLI Interface",
        "description": "Create the command-line interface for all commands with proper argument parsing and output formatting.",
        "details": "1. Create cli/main.py:\n   - Implement commands: generate, analyze, coverage, status\n   - Add utility commands: init-config, env, cost, debug-state, sync-state, reset-state\n   - Support flags: batch-size, streaming, force, dry-run, model options, etc.\n   - Format output with concise tables, spinners, summaries\n   - Add verbose mode for detailed diagnostics\n2. Create cli/config_init.py:\n   - Generate full YAML with commented guidance\n   - Provide minimal preset option\n3. Implement dependency injection for use cases\n4. Add proper error handling and user-friendly messages\n5. Support plugin discovery via entry points",
        "testStrategy": "Unit tests for command-line argument parsing and validation. Test output formatting with various results. Verify error handling provides useful messages. Test config initialization with different options.",
        "priority": "medium",
        "dependencies": [
          3,
          15,
          16,
          17,
          18
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Implement Evaluation Harness and Documentation",
        "description": "Create an evaluation system for testing the quality of generated tests and comprehensive documentation.",
        "details": "1. Create evaluation/harness.py:\n   - Implement offline golden repos testing\n   - Add acceptance checks: syntactic validity, importability, pytest success, coverage improvements\n   - Support LLM-as-judge with rubric for test quality (optional)\n   - Enable SxS prompt variants A/B testing\n   - Store prompt registry version in artifacts\n2. Create comprehensive documentation:\n   - README.md with quickstart guide\n   - Advanced usage documentation\n   - Configuration reference\n   - Architecture overview\n   - Contributing guidelines\n3. Create sample .testgen.yml files:\n   - Minimal configuration\n   - Comprehensive configuration with comments\n4. Set up CI pipeline:\n   - Lint with ruff\n   - Type-check with mypy\n   - Run tests with pytest\n   - Verify documentation builds\n<info added on 2025-09-08T12:59:20.901Z>\nLatest trends and best practices for LLM-as-judge evaluation systems (2025):\n\nIntegrate a rubric-driven LLM-as-judge workflow into evaluation/harness.py, leveraging explicit, versioned rubrics for test quality dimensions such as correctness, coverage, clarity, and safety. Implement prompt engineering to instruct the LLM judge to provide both numeric scores and concise rationales for each dimension, storing all scores, rationales, and prompt versions as structured artifacts for traceability and auditability. Support both single-output scoring and pairwise (A/B) comparison for prompt variant evaluation, with standardized prompts to mitigate bias and ensure repeatability. For reliability, optionally aggregate scores from multiple LLM judges and rerun evaluations to detect inconsistencies. Combine LLM-based semantic assessment with automated acceptance checks (syntactic validity, importability, pytest success, coverage delta) to detect hallucinations and subtle bugs. Document the evaluation methodology, including rubric definitions, prompt templates, and artifact storage patterns, in README and advanced usage docs. Recommended frameworks and patterns include Langfuse for rubric-driven scoring and rationale logging, MT-Bench/Chatbot Arena for pairwise comparison, and custom golden repo pipelines for offline evaluation. Monitor and audit LLM judge outputs for consistency, especially when updating prompts or switching LLM providers.\n</info added on 2025-09-08T12:59:20.901Z>\n<info added on 2025-09-08T13:00:27.355Z>\nAppend the following best practices and implementation guidance for prompt A/B testing and evaluation:\n\nIntegrate a systematic, multi-layered prompt evaluation pipeline into evaluation/harness.py, following 2025 best practices:\n\n- Implement side-by-side (SxS) A/B testing by executing all prompt variants on curated, representative datasets that include real-world cases and edge scenarios.\n- Support both automated LLM-based scoring (LLM-as-judge) and human-in-the-loop review. Use explicit, versioned rubrics for dimensions such as correctness, coverage, clarity, and safety. Instruct LLM judges to provide numeric scores and concise rationales for each dimension, storing all results and prompt versions as structured artifacts for traceability.\n- Enable pairwise (A/B) comparison workflows, using standardized prompts to minimize bias and ensure repeatability. Optionally aggregate scores from multiple LLM judges and rerun evaluations to detect inconsistencies.\n- Incorporate statistical significance testing (e.g., t-tests, bootstrap) to determine if observed differences between prompt variants are meaningful.\n- Combine LLM-based semantic assessment with automated acceptance checks (syntactic validity, importability, pytest success, coverage delta) to detect hallucinations and subtle bugs.\n- Continuously monitor prompt performance on live data and feed results back into the evaluation loop for ongoing optimization.\n- Document the evaluation methodology, including rubric definitions, prompt templates, and artifact storage patterns, in README and advanced usage docs.\n\nRecommended tools and frameworks for systematic prompt evaluation and optimization include:\n- Helicone (prompt analytics, A/B testing, production monitoring)\n- OpenAI Eval (custom evals, LLM-as-judge integration)\n- PromptFoo (prompt variant testing, regression tracking)\n- PromptLayer (version control, analytics, human-in-the-loop)\n- Agenta (side-by-side LLM comparisons)\n- LangChain (modular evaluation chains)\n- OpenPrompt (advanced templates, dynamic evaluation)\n- Traceloop (prompt tracing, debugging)\n- Braintrust (collaborative human review)\n- Langfuse (rubric-driven scoring and rationale logging)\n- MT-Bench/Chatbot Arena (pairwise comparison frameworks)\n\nEnsure the evaluation harness supports:\n- Batch execution of prompt variants and logging of all input/output pairs, variant IDs, and evaluation scores via the state management system.\n- CLI commands for running A/B tests, viewing evaluation results, and comparing prompt performance, with Rich-based UI for displaying evaluation tables and statistical summaries.\n- Multimodal prompt evaluation if needed (e.g., OpenPrompt, LangChain).\n- Integration of prompt evaluation into the CI/CD pipeline for automatic regression testing on prompt changes.\n- Regular bias and fairness audits using both automated and human review.\n\nInclude a best practices checklist in documentation covering dataset curation, side-by-side prompt runs, automated and human evaluation, statistical testing, state logging, continuous monitoring, and multimodal support.\n</info added on 2025-09-08T13:00:27.355Z>",
        "testStrategy": "Test the evaluation harness with sample repositories. Verify documentation is accurate and comprehensive. Test CI pipeline with various code changes to ensure it catches issues.",
        "priority": "low",
        "dependencies": [
          17,
          19
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Design and Implement testcraft Evaluation Harness Core",
            "description": "Develop evaluation/harness.py for testcraft, supporting offline golden repo testing, automated acceptance checks (syntactic validity, importability, pytest success, coverage delta), and integration with JSON state adapters and existing LLM adapters. Ensure clean architecture and artifact storage patterns.",
            "dependencies": [],
            "details": "Implement the core harness logic in evaluation/harness.py, using the testcraft naming convention and TOML-based configuration (.testcraft.toml). Integrate with the coverage adapters and LLM adapters, and ensure all evaluation artifacts (inputs, outputs, scores, rationales, prompt versions) are stored as structured JSON for traceability. Follow clean architecture principles to separate evaluation logic, adapters, and artifact management.\n<info added on 2025-09-08T13:13:40.132Z>\n✅ Subtask 20.1 Implementation Complete!\n\nWhat was accomplished:\n\n1. Created EvaluationPort Interface (testcraft/ports/evaluation_port.py):\n   - Comprehensive protocol defining all evaluation operations\n   - Rich type definitions: EvaluationConfig, AcceptanceResult, LLMJudgeResult, EvaluationResult\n   - Support for single, pairwise, and batch evaluation modes\n   - Golden repository evaluation and trend analysis methods\n   - Follows testcraft's established port-based architecture patterns\n\n2. Implemented TestcraftEvaluationAdapter (testcraft/adapters/evaluation/main_adapter.py):\n   - Automated Acceptance Checks: Syntax validation, import checking, pytest execution, coverage improvement measurement\n   - LLM-as-Judge Integration: Rubric-driven evaluation with structured prompts, rationale generation, versioned prompts from registry\n   - A/B Testing Pipeline: Pairwise comparison with statistical confidence, side-by-side evaluation\n   - Batch Processing: Efficient evaluation of multiple test variants\n   - Golden Repository Testing: Comprehensive regression detection against known-good repositories\n   - Trend Analysis: Historical evaluation analysis with recommendations\n   - Clean Architecture: Proper dependency injection via ports, no direct adapter dependencies\n   - 2025 Best Practices: Bias mitigation, statistical testing, artifact storage with traceability\n\n3. Created Evaluation Harness (evaluation/harness.py):\n   - High-level Interface: Convenient methods for all evaluation scenarios\n   - Dependency Management: Automatic initialization of required adapters\n   - Configuration Support: Integration with testcraft config system\n   - Convenience Functions: Quick evaluation and comparison utilities\n   - Proper Integration: Uses JSON state adapters, artifact storage, LLM routing\n\nKey Features Implemented:\n- testcraft naming throughout (not testgen)\n- Clean architecture with port-based dependency injection\n- JSON state management integration for evaluation artifacts\n- Safety policies enforcement for file operations\n- Comprehensive error handling and logging\n- Artifact storage for all evaluation results with cleanup policies\n- LLM-as-judge with rubric-driven scoring and rationale generation\n- Statistical A/B testing with confidence estimation\n- Golden repo evaluation for regression testing\n- Trend analysis for continuous improvement\n\nIntegration Status:\n- Coverage adapter integration for acceptance checks\n- LLM adapter integration via router for flexibility  \n- State adapter integration for persistent evaluation tracking\n- Artifact store integration for result storage and cleanup\n- Prompt registry integration for versioned evaluation prompts\n- Safety policies integration for secure file operations\n\nThe core evaluation harness is now fully operational and ready for integration testing. All components follow established testcraft patterns and support the latest 2025 evaluation methodologies.\n</info added on 2025-09-08T13:13:40.132Z>",
            "status": "done",
            "testStrategy": "Test with sample repositories and golden repos. Verify that all acceptance checks run correctly and that artifacts are stored in the expected JSON structure. Use unit and integration tests to validate harness behavior and error handling."
          },
          {
            "id": 2,
            "title": "Integrate Rubric-Driven LLM-as-Judge and A/B Testing Pipeline",
            "description": "Implement rubric-driven LLM-as-judge workflows with rationale generation, versioned prompt registry, and support for both single-output and pairwise (A/B) scoring. Integrate bias mitigation, statistical significance testing, and human-in-the-loop review. Leverage modern frameworks (PromptFoo, PromptLayer patterns) and enable batch prompt variant execution.",
            "dependencies": [],
            "details": "Add LLM-as-judge evaluation with explicit, versioned rubrics for correctness, coverage, clarity, and safety. Engineer prompts to elicit numeric scores and concise rationales per dimension. Store all scores, rationales, and prompt versions as structured artifacts. Implement side-by-side (SxS) A/B testing with standardized prompts and statistical significance testing (e.g., t-tests, bootstrap). Support aggregation of multiple LLM judges and human review. Integrate with PromptFoo/PromptLayer-style pipelines and ensure CLI commands for running and visualizing A/B tests.",
            "status": "done",
            "testStrategy": "Test with multiple prompt variants and sample datasets, verifying correct rubric application, rationale logging, and artifact storage. Validate statistical testing outputs and aggregation logic. Simulate bias and verify mitigation strategies. Confirm human-in-the-loop review can be triggered and logged."
          },
          {
            "id": 3,
            "title": "Develop TOML-Based Configuration System and Sample .testcraft.toml Files",
            "description": "Implement a TOML configuration system for testcraft, replacing YAML. Provide minimal and comprehensive .testcraft.toml samples with detailed comments and schema validation.",
            "dependencies": [],
            "details": "Design a robust TOML schema for all evaluation harness options, including LLM-as-judge settings, prompt registry, acceptance checks, and artifact paths. Generate sample .testcraft.toml files: one minimal for quickstart, one comprehensive with inline comments explaining each option. Ensure schema validation and error reporting for misconfigurations.",
            "status": "done",
            "testStrategy": "Validate configuration parsing with both minimal and comprehensive TOML files. Test error handling for invalid configs. Confirm all harness features are configurable via TOML and that changes are reflected in harness behavior."
          },
          {
            "id": 4,
            "title": "Implement CI Pipeline for Evaluation and Documentation Quality",
            "description": "Set up a CI pipeline to lint (ruff), type-check (mypy), run tests (pytest), verify documentation builds, and automate regression testing for prompt and evaluation changes.",
            "dependencies": [],
            "details": "Configure CI to run ruff for linting, mypy for type checks, and pytest for all evaluation harness and adapter tests. Add steps to build and check documentation. Integrate prompt evaluation into CI/CD: automatically run regression tests on prompt changes, log evaluation results, and fail builds on significant regressions. Ensure all artifacts and logs are stored for traceability.",
            "status": "done",
            "testStrategy": "Trigger CI on code and config changes. Verify that lint, type, and test failures are caught. Simulate prompt changes and confirm regression tests run and results are logged. Ensure documentation builds without errors."
          },
          {
            "id": 5,
            "title": "Author Comprehensive Documentation and Best Practices Checklist",
            "description": "Write and maintain README.md, advanced usage docs, configuration reference, architecture overview, contributing guidelines, and a best practices checklist for prompt evaluation and harness usage.",
            "dependencies": [],
            "details": "Document all evaluation harness features, configuration options, and architecture. Include a quickstart guide, advanced usage (LLM-as-judge, A/B testing, statistical analysis), and a configuration reference for .testcraft.toml. Provide an architecture overview and contributing guidelines. Add a best practices checklist covering dataset curation, prompt evaluation, statistical testing, artifact logging, continuous monitoring, and multimodal support.\n<info added on 2025-09-08T14:29:08.497Z>\n✅ Subtask 20.5 Implementation Complete!\n\nSuccessfully authored comprehensive documentation and best practices checklist for TestCraft evaluation harness:\n\nDocumentation Deliverables Created:\n\n1. Updated README.md \n- Complete quickstart guide with evaluation harness features\n- Beautiful architecture overview with emojis and clear structure\n- Advanced evaluation and A/B testing command examples\n- Updated installation, configuration, and development sections\n- Links to all new documentation resources\n\n2. Advanced Usage Guide (docs/advanced-usage.md)\n- Comprehensive evaluation harness usage patterns\n- A/B testing and prompt optimization workflows  \n- Statistical analysis methodology and interpretation\n- Bias detection and mitigation strategies\n- Golden repository testing procedures\n- Comprehensive evaluation campaign management\n- Configuration management best practices\n- Integration patterns for CI/CD and custom adapters\n- Real-world code examples throughout\n\n3. Configuration Reference (docs/configuration.md)\n- Complete TOML configuration documentation\n- All evaluation harness configuration options\n- Environment variable override patterns\n- Security and performance configuration\n- LLM provider setup and model selection\n- Validation and troubleshooting guidance\n- Best practices for environment-specific configs\n\n4. Architecture Overview (docs/architecture.md)\n- Clean Architecture principles and implementation\n- Detailed component breakdown and interactions\n- Evaluation harness architecture deep-dive\n- Dependency injection patterns\n- Error handling strategies\n- Testing architecture and patterns\n- Extension points for customization\n- Code examples demonstrating patterns\n\n5. Contributing Guidelines (CONTRIBUTING.md)\n- Complete contributor onboarding guide\n- Code style and architecture standards\n- Testing requirements and patterns\n- Pull request process and templates\n- Issue reporting guidelines\n- Evaluation system contribution guidelines\n- Release process documentation\n- Community guidelines and recognition\n\n6. Best Practices Checklist (docs/best-practices-checklist.md)\n- Comprehensive checklist covering all evaluation aspects:\n  - Dataset Curation: Data collection, validation, versioning\n  - Prompt Evaluation: Multi-dimensional assessment, LLM-as-judge\n  - Statistical Testing: Methodology, quality control, interpretation\n  - Artifact Logging: Storage, retention, reproducibility\n  - Continuous Monitoring: Quality trends, anomaly detection, KPIs\n  - Bias Detection: Identification, metrics, mitigation strategies\n  - A/B Testing: Campaign planning, execution, management\n  - Configuration Management: Environment separation, validation\n  - Performance Optimization: System performance, cost optimization\n  - Security and Privacy: Data protection, code safety\n  - Multimodal Support: Content type handling, cross-modal evaluation\n  - Quality Assurance: Testing validation, documentation standards\n\nKey Features Documented:\n\nEvaluation Harness Core Functions:\n- Single test evaluation with acceptance checks\n- LLM-as-judge with rubric-driven scoring\n- Batch evaluation and A/B testing pipelines\n- Statistical significance analysis\n- Bias detection and mitigation\n- Golden repository regression testing\n\nAdvanced Workflows:\n- Comprehensive evaluation campaigns\n- Cross-scenario analysis and recommendations\n- Trend analysis and continuous monitoring\n- Integration patterns for CI/CD\n- Custom adapter development\n\nConfiguration System:\n- TOML-based hierarchical configuration\n- Environment-specific setups\n- Security and performance optimization\n- Complete option reference\n\nBest Practices Coverage:\n- Dataset curation and quality assurance\n- Statistical methodology and interpretation\n- Artifact management and reproducibility\n- Continuous monitoring and alerting\n- Bias detection and fairness assessment\n- Performance and cost optimization\n\nDocumentation Quality Standards Met:\n\n- Comprehensive Coverage: All evaluation features documented\n- Practical Examples: Real code examples throughout\n- Multiple Skill Levels: Beginner quickstart + advanced patterns\n- Cross-Referenced: Extensive linking between documents\n- Actionable Guidance: Step-by-step procedures and checklists\n- Best Practices: Industry-standard evaluation methodologies\n- Maintainable: Clear structure for ongoing updates\n\nThe documentation suite provides complete coverage of TestCraft's evaluation capabilities following 2025 best practices for LLM evaluation, A/B testing, and statistical analysis. Users can now effectively implement comprehensive test quality assessment workflows with proper bias detection, statistical validation, and continuous improvement processes.\n</info added on 2025-09-08T14:29:08.497Z>",
            "status": "done",
            "testStrategy": "Review documentation for completeness and clarity. Test quickstart and advanced usage steps on a fresh environment. Validate that the best practices checklist is actionable and covers all critical evaluation steps."
          }
        ]
      },
      {
        "id": 21,
        "title": "Implement Rich-based UI System for CLI",
        "description": "Create a comprehensive UI system using the Rich library to provide beautiful, interactive command-line interfaces with progress bars, tables, panels, syntax highlighting, and interactive components.",
        "details": "1. Create adapters/ui/rich_adapter.py implementing UIPort:\n   - Implement the UIPort interface defined in Task 4\n   - Create a RichAdapter class with methods for all required UI operations\n   - Configure Rich theme settings with consistent color schemes\n   - Add support for terminal width detection and responsive layouts\n   - Implement graceful fallbacks for terminals without Rich support\n\n2. Implement Rich UI Components:\n   - Create ui/components/ directory with reusable Rich components:\n     - TestResultsTable: Display test generation results with syntax highlighting\n     - CoverageReportPanel: Show coverage metrics with color-coded indicators\n     - ProgressTracker: Display progress for multi-step operations\n     - ErrorDisplay: Format errors with traceback highlighting\n     - ConfigurationWizard: Interactive setup with form inputs\n   - Ensure components are themeable and configurable\n\n3. Implement Progress Visualization:\n   - Create display_progress() with Rich progress bars\n   - Support for nested progress tracking (e.g., file-level and test-level)\n   - Add spinners for indeterminate operations\n   - Implement ETA calculations for long-running tasks\n   - Support for cancellation and pause/resume indicators\n\n4. Implement Results Display:\n   - Create display_results() with Rich tables and panels\n   - Format test results with syntax highlighting for code snippets\n   - Color-code success/failure states\n   - Support for collapsible sections for detailed information\n   - Add summary statistics at the top of reports\n\n5. Add Integration Features:\n   - Support both verbose and quiet output modes\n   - Implement proper terminal capabilities detection\n   - Create plain text fallback renderer for non-interactive environments\n   - Integrate with the existing logging system\n   - Support output redirection and piping\n   - Add configuration options for UI preferences",
        "testStrategy": "1. Unit Tests:\n   - Test RichAdapter implementation against the UIPort interface\n   - Verify each UI component renders correctly with different inputs\n   - Test responsive layout with various terminal widths\n   - Verify fallback mechanisms work when Rich features are unavailable\n   - Test integration with the logging system\n\n2. Integration Tests:\n   - Create mock test generation scenarios and verify UI components display correctly\n   - Test progress tracking with simulated long-running operations\n   - Verify error display with various error types\n   - Test interactive components with simulated user input\n\n3. Visual Verification:\n   - Create a test script that demonstrates all UI components\n   - Capture screenshots of UI components for documentation\n   - Verify color schemes are consistent across components\n   - Test with different terminal types and color schemes\n\n4. Edge Cases:\n   - Test with extremely large datasets to verify table pagination\n   - Verify behavior when terminal is resized during operation\n   - Test with redirected output and in CI environments\n   - Verify accessibility considerations (color contrast, etc.)",
        "status": "done",
        "dependencies": [
          4,
          19
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Integrate Real LLM APIs and Fix Adapter Interface Issues",
        "description": "Replace all fake LLM adapter implementations with real API integrations (at minimum OpenAI), resolve method signature mismatches, and ensure all adapters are production-ready for real use.",
        "details": "1. Replace all mock/fake LLM adapters with real API integrations, starting with OpenAI using the official Python SDK (`openai` package). Ensure API keys are handled securely via environment variables or configuration files, never hardcoded. Example for OpenAI:\n\n```python\nimport openai\nimport os\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n```\n\n2. Implement robust API key management and configuration, leveraging the existing configuration system. Validate that keys are loaded at runtime and provide clear error messages if missing.\n\n3. Refactor the RefineAdapter and any other LLM adapters to ensure their method signatures match the expected interface. Specifically, ensure that calls to `llm.generate()` are replaced with the correct methods (`generate_tests()`, `analyze_code()`, `refine_content()`) as defined in the adapters. Update all usages and tests accordingly.\n\n4. Audit all LLM adapters for interface mismatches and correct them to ensure consistency across providers (OpenAI, Claude, Azure, Bedrock, etc.).\n\n5. Integrate real prompt construction using the prompt registry, ensuring that all LLM calls use the correct, versioned prompt templates and that prompt formatting is robust.\n\n6. Implement and test real API calls, handling JSON parsing, schema validation, and error handling for real-world LLM responses (including malformed or partial outputs). Use the common helpers for response parsing and validation.\n\n7. Ensure that response validation logic is robust against real LLM outputs, not just mock data. Update or extend JSON schema validation as needed.\n\n8. Update any other adapters or modules that depend on LLM functionality to use the new, real implementations. Remove or clearly separate any remaining mock/test-only code.\n\n9. Document all changes, including configuration instructions for API keys and troubleshooting for common integration errors.\n<info added on 2025-09-07T22:49:49.617Z>\nOfficial Python SDKs for 2024 are: OpenAI (`openai`), Anthropic Claude (`anthropic`), Azure OpenAI (via `openai` with Azure config), and AWS Bedrock (`boto3`). All are actively maintained, support type hints, and most offer async and streaming capabilities. For authentication, OpenAI and Anthropic use API keys loaded via environment variables or secure config; Azure OpenAI requires both an API key and endpoint; AWS Bedrock uses AWS credentials (environment variables, profiles, or IAM roles). Best practices include never hardcoding secrets, using Pydantic models for config validation, dependency injection for adapter setup, robust error handling with retries and logging, and output validation against JSON schemas. Adapters should abstract provider-specific logic behind a unified interface, support both sync and async methods, and handle streaming where available. Integration tests should use real API keys and validate outputs; unit tests should mock responses. Ensure adapters are extensible for future providers and keep SDKs updated to avoid breaking changes.\n</info added on 2025-09-07T22:49:49.617Z>\n<info added on 2025-09-07T22:50:54.347Z>\nThe latest official Python SDKs for LLM integration as of September 2025 are: OpenAI (`openai`, v1.102.0), Anthropic Claude (`anthropic`, v0.21.0), Azure OpenAI (`azure-ai-openai`, v1.2.0), and AWS Bedrock (`boto3`, v1.34.x). All SDKs support Python 3.8+, provide both synchronous and asynchronous API access, and offer type-safe request/response objects. Authentication methods are provider-specific: OpenAI and Anthropic use API keys loaded via environment variables or secure config; Azure OpenAI requires both an API key and endpoint with Azure AD credentials; AWS Bedrock uses AWS credentials via environment variables, profiles, or IAM roles. Production best practices include never hardcoding secrets, using environment variables or secret managers, implementing robust error handling and retries, logging request/response metadata, and validating all LLM outputs against JSON schemas. Adapters should abstract provider-specific logic behind a unified interface, support both sync and async methods, and handle streaming where available. Integration tests must use real API keys/accounts and validate outputs; unit tests should mock SDK responses. Ensure adapters are extensible for future providers and keep SDKs updated to avoid breaking changes. Update all LLM adapters to use the latest SDKs and authentication methods, refactor method signatures to match SDK requirements and the `LLMPort` protocol, and implement secure credential loading via the configuration system.\n</info added on 2025-09-07T22:50:54.347Z>",
        "testStrategy": "1. Write integration tests that perform real API calls to OpenAI (and other providers if possible) using test API keys, verifying that the adapters return valid, correctly parsed responses.\n2. Validate that all LLM outputs conform to the expected JSON schema using the existing validation logic.\n3. Simulate and test error handling for common API failures (invalid key, rate limit, malformed response, network errors).\n4. Run end-to-end flows (e.g., test generation, refinement) to ensure the system works with real LLMs and not just mock data.\n5. Confirm that all method signatures and interfaces are consistent and that no calls to removed or renamed methods remain.\n6. Review logs and error messages for clarity and completeness.\n7. Ensure that configuration and API key handling works in various environments (local, CI, production).",
        "status": "done",
        "dependencies": [
          3,
          11,
          13,
          14
        ],
        "priority": "high",
        "subtasks": [
          {
            "id": 1,
            "title": "Replace Mock LLM Adapters with Real API Integrations",
            "description": "Remove all mock or fake LLM adapter implementations and integrate real API calls using the latest official Python SDKs for OpenAI, Anthropic Claude, Azure OpenAI, and AWS Bedrock. Ensure adapters use the correct SDK versions and support both synchronous and asynchronous methods as required.",
            "dependencies": [],
            "details": "Install and configure the latest SDKs: openai (v1.102.0), anthropic (v0.21.0), azure-ai-openai (v1.2.0), and boto3 (v1.34.x). Refactor adapter code to use these SDKs for all LLM operations, abstracting provider-specific logic behind a unified interface. Ensure adapters are extensible for future providers and support streaming where available.\n<info added on 2025-09-07T22:55:08.907Z>\nUpdate SDK versions to use openai (v1.106.1, released Sep 4, 2025) and anthropic (v0.66.0) as the latest supported versions. Ensure all adapter code and dependencies reflect these versions, and verify that pyproject.toml is updated accordingly.\n</info added on 2025-09-07T22:55:08.907Z>\n<info added on 2025-09-07T22:57:55.388Z>\nUpdate AWS Bedrock integration to use the ChatBedrock class from the langchain-aws package instead of direct boto3 calls. This approach ensures a more consistent interface across providers, leverages built-in retry logic and error handling, and enables advanced features such as prompt caching. Add langchain-aws as a dependency and update all relevant adapter code and configuration to utilize ChatBedrock for Bedrock LLM operations.\n</info added on 2025-09-07T22:57:55.388Z>",
            "status": "done",
            "testStrategy": "Write integration tests that perform real API calls to each provider using test API keys/accounts. Validate that adapters return valid, correctly parsed responses for all supported operations."
          },
          {
            "id": 2,
            "title": "Implement Secure API Key and Credential Management",
            "description": "Integrate robust API key and credential management for all LLM providers, leveraging the existing configuration system. Ensure secrets are never hardcoded and are loaded securely at runtime.",
            "dependencies": [
              "22.1"
            ],
            "details": "Use environment variables or secret managers for OpenAI and Anthropic API keys, Azure OpenAI API key and endpoint, and AWS credentials (environment variables, profiles, or IAM roles). Validate credentials at startup and provide clear error messages if missing. Use Pydantic models for config validation and dependency injection for adapter setup.",
            "status": "done",
            "testStrategy": "Test configuration loading with valid and missing credentials for each provider. Verify that errors are raised and logged appropriately when credentials are absent or invalid."
          },
          {
            "id": 3,
            "title": "Refactor and Standardize Adapter Interfaces and Method Signatures",
            "description": "Audit all LLM adapters for interface mismatches and refactor method signatures to match the unified LLMPort protocol and SDK requirements. Ensure all calls use the correct methods and update usages and tests accordingly.",
            "dependencies": [
              "22.1"
            ],
            "details": "Update RefineAdapter and other adapters to expose methods like generate_tests(), analyze_code(), and refine_content() as defined in the interface. Replace direct llm.generate() calls with the appropriate adapter methods. Ensure consistency across all providers and update all usages and tests to match the new signatures.",
            "status": "done",
            "testStrategy": "Run static type checks and unit tests to verify that all adapters conform to the expected interface. Add tests for each method to ensure correct invocation and output structure."
          },
          {
            "id": 4,
            "title": "Integrate Prompt Registry and Robust Prompt Construction",
            "description": "Ensure all LLM adapters use the prompt registry for constructing prompts, utilizing versioned prompt templates and robust formatting. Validate that prompt construction is consistent and resilient to template changes.",
            "dependencies": [
              "22.3"
            ],
            "details": "Refactor adapters to fetch and format prompts using the prompt registry. Implement logic to select the correct prompt version and handle formatting errors gracefully. Ensure all LLM calls use the constructed prompts and that prompt changes are tracked and versioned.",
            "status": "done",
            "testStrategy": "Write tests that verify correct prompt selection, formatting, and usage for each adapter method. Simulate prompt template changes and validate adapter robustness."
          },
          {
            "id": 5,
            "title": "Implement and Validate Real LLM Response Parsing and Output Validation",
            "description": "Implement robust parsing, schema validation, and error handling for real LLM API responses. Ensure adapters handle malformed or partial outputs and validate all outputs against JSON schemas.",
            "dependencies": [
              "22.1",
              "22.3"
            ],
            "details": "Use common helpers for response parsing and validation. Extend or update JSON schema validation logic as needed to handle real-world LLM outputs. Implement error handling with retries and logging for API failures or malformed responses. Remove or clearly separate any remaining mock/test-only code.",
            "status": "done",
            "testStrategy": "Write integration tests that send real requests and validate outputs against schemas. Test error handling by simulating malformed, partial, or invalid responses. Ensure all adapters reject or correct invalid outputs and log errors appropriately."
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-01-27T20:16:58.061Z",
      "updated": "2025-09-08T14:29:25.245Z",
      "description": "Tasks for master context"
    }
  }
}