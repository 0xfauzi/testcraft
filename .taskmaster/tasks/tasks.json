{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Project Scaffolding and Environment Setup",
        "description": "Set up the initial project structure with Python 3.11+, development tools, and directory layout according to the PRD specifications.",
        "details": "1. Create a new Python project with Python 3.11+ support\n2. Set up development tools: uv, ruff, black, mypy, pytest\n3. Create directory structure following the Clean/Hex architecture:\n   - domain/\n   - application/\n   - adapters/\n   - ports/\n   - cli/\n4. Initialize pyproject.toml with entry points: `sloptest = smart_test_generator.cli.main:app`\n5. Set up .gitignore, README.md, and LICENSE files\n6. Configure development environment with virtual environment\n7. Create initial package structure with __init__.py files",
        "testStrategy": "Verify project structure exists with correct directories. Ensure all development tools can be invoked. Validate pyproject.toml configuration with a simple import test.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Python Project and Virtual Environment",
            "description": "Create a new Python project directory, ensure Python 3.11+ is available, and set up a virtual environment for isolated development.",
            "dependencies": [],
            "details": "Establish the root project folder, verify Python 3.11+ installation, and create a virtual environment using the preferred tool (e.g., venv, uv, or Poetry). Activate the environment for subsequent steps.",
            "status": "done",
            "testStrategy": "Check that the virtual environment is active and Python version is 3.11 or higher by running 'python --version' and verifying isolation from global packages."
          },
          {
            "id": 2,
            "title": "Configure Development Tools and Linting",
            "description": "Install and configure development tools: uv (or Poetry/PDM), ruff, black, mypy, and pytest for code formatting, linting, type checking, and testing.",
            "dependencies": [
              "1.1"
            ],
            "details": "Use the package manager to install ruff, black, mypy, and pytest as development dependencies. Create or update configuration files for each tool (e.g., pyproject.toml sections or standalone config files) to enforce code quality standards.\n<info added on 2025-09-06T21:22:00.312Z>\nProject name has been updated to \"testcraft\" from \"smart-test-generator\". The pyproject.toml file has been modified to reflect this change, including updating the project name and adjusting the pytest coverage path to match the new structure. All development tools (ruff, black, mypy, pytest) have been successfully configured with appropriate settings and verified to be working correctly.\n</info added on 2025-09-06T21:22:00.312Z>",
            "status": "done",
            "testStrategy": "Run each tool (ruff, black, mypy, pytest) on a sample file to confirm correct installation and configuration."
          },
          {
            "id": 3,
            "title": "Establish Project Directory Structure (Clean/Hex Architecture)",
            "description": "Create the core directory layout: domain/, application/, adapters/, ports/, cli/, and ensure each contains an __init__.py file for package recognition.",
            "dependencies": [
              "1.1"
            ],
            "details": "Manually or via script, generate the specified directories and add empty __init__.py files to each. Follow Clean/Hex architecture conventions for separation of concerns.",
            "status": "done",
            "testStrategy": "Verify the presence of all required directories and __init__.py files. Attempt to import each package in a Python shell to confirm discoverability."
          },
          {
            "id": 4,
            "title": "Initialize pyproject.toml and Project Metadata",
            "description": "Create and configure pyproject.toml with project metadata, dependencies, tool configurations, and entry points as specified.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Define project name, version, authors, and dependencies in pyproject.toml. Add entry point: 'sloptest = smart_test_generator.cli.main:app'. Include tool configuration sections for ruff, black, mypy, and pytest as needed.",
            "status": "done",
            "testStrategy": "Validate pyproject.toml syntax and confirm entry point is discoverable by running 'python -m smart_test_generator.cli.main' or equivalent."
          },
          {
            "id": 5,
            "title": "Add Essential Project Files and Version Control",
            "description": "Create .gitignore, README.md, and LICENSE files. Initialize a Git repository and make the initial commit.",
            "dependencies": [
              "1.1",
              "1.3",
              "1.4"
            ],
            "details": "Draft a .gitignore tailored for Python projects, write a basic README.md with project overview and setup instructions, and select an appropriate open-source LICENSE. Initialize Git and commit all scaffolding files.",
            "status": "done",
            "testStrategy": "Verify that .gitignore excludes virtual environment and build artifacts, README.md renders correctly on GitHub, LICENSE is present, and 'git status' shows a clean working directory after commit."
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Domain Models",
        "description": "Create the core domain models using pydantic to represent the fundamental entities in the system.",
        "status": "done",
        "dependencies": [
          1
        ],
        "priority": "high",
        "details": "1. Create domain/models.py with the following models:\n   - TestGenerationPlan (elements_to_test, existing_tests, coverage_before)\n   - TestElement (name, type, line_range, docstring)\n   - CoverageResult (line_coverage, branch_coverage, missing_lines)\n   - GenerationResult (file_path, content, success, error_message)\n   - RefineOutcome (updated_files, rationale, plan)\n   - AnalysisReport (files_to_process, reasons, existing_test_presence)\n2. Use pydantic for validation and serialization\n3. Implement proper type hints for all models\n4. Add docstrings explaining each model's purpose and fields\n5. Ensure models are immutable where appropriate\n6. Implement TestElementType enum for categorizing test elements (function, class, method, module)\n7. Add custom validators for data integrity:\n   - Line ranges must be valid (start <= end, positive numbers)\n   - Coverage percentages must be between 0.0 and 1.0\n   - Missing lines are sorted and deduplicated\n   - Error messages required when generation fails\n   - All required mappings must cover all files in lists",
        "testStrategy": "Unit tests for each model verifying initialization, validation rules, serialization/deserialization, and edge cases like empty values or invalid inputs. Specifically test custom validators for line ranges, coverage percentages, and other data integrity rules.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement TestElement model",
            "description": "Create TestElement model with name, type, line range, and docstring fields",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement CoverageResult model",
            "description": "Create CoverageResult model with line/branch coverage and missing lines",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement GenerationResult model",
            "description": "Create GenerationResult model with file path, content, success/failure status and error message",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement TestGenerationPlan model",
            "description": "Create TestGenerationPlan model with elements to test, existing tests, and coverage information",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement RefineOutcome model",
            "description": "Create RefineOutcome model with updated files, rationale and plan",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement AnalysisReport model",
            "description": "Create AnalysisReport model with files to process, reasons, and existing test presence",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Implement TestElementType enum",
            "description": "Create enum for categorizing test elements (function, class, method, module)",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Add custom validators",
            "description": "Implement validators for line ranges, coverage percentages, missing lines, and other data integrity rules",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Configure model immutability",
            "description": "Set models as immutable using `frozen = True` configuration",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Add field descriptions",
            "description": "Add detailed field descriptions for better API documentation",
            "status": "completed",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Configuration System",
        "description": "Create a typed configuration system using pydantic that validates on load, provides sensible defaults, and supports all required configuration segments.",
        "details": "1. Create config/model.py with pydantic models for each configuration segment:\n   - TestGenerationConfig (minimum_line_coverage, etc.)\n   - CoverageConfig (minimum_line_coverage)\n   - MergeConfig (strategy: Literal[\"append\", \"ast-merge\"])\n   - TestRunnerConfig (enable: bool)\n   - RefineConfig (enable: bool, max_retries, backoff, caps)\n   - ContextConfig (retrieval settings, hybrid weights, rerank model, hyde: bool)\n   - SecurityConfig (block_patterns: list[str], max_generated_file_size)\n   - CostConfig (daily_limit, per_request_limit)\n   - EnvironmentConfig\n   - QualityConfig\n   - PromptEngineeringConfig\n2. Create config/loader.py to merge YAML+env+CLI sources\n3. Implement validation logic for all configuration parameters\n4. Add sensible defaults for all non-required fields\n5. Support environment variable overrides with prefix\n6. Add helper methods for accessing nested configuration",
        "testStrategy": "Unit tests verifying configuration loading from different sources, validation of required fields, default values, environment variable overrides, and error handling for invalid configurations.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Define Interface Ports",
        "description": "All 12 interface ports have been implemented using Python Protocols, providing clear contracts between the application layer and adapters. Each port is defined in its own file under testcraft/ports/, with comprehensive docstrings, precise type hints, and no adapter imports. The __init__.py file exports all port interfaces. These protocols are now ready for adapter implementations.",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "details": "1. 12 protocol interface files created in testcraft/ports/:\n   - LLMPort: Large Language Model operations (test generation, analysis, refinement)\n   - CoveragePort: Code coverage measurement and reporting\n   - WriterPort: File writing operations (test files, reports)\n   - ParserPort: Source code parsing and AST analysis\n   - ContextPort: Context management (indexing, retrieval, summarization)\n   - PromptPort: Prompt management (system/user prompts, schemas)\n   - RefinePort: Test refinement operations\n   - StatePort: Application state management\n   - ReportPort: Report generation (analysis, coverage, summaries)\n   - UIPort: User interface operations (progress, results, input)\n   - CostPort: Cost tracking and usage monitoring\n   - TelemetryPort: Observability and metrics collection\n2. All protocols use Python's typing.Protocol for interface definitions\n3. Comprehensive docstrings provided for each method\n4. All parameters and return values have precise type hints\n5. No imports from adapters in any port file\n6. __init__.py updated to export all port interfaces\n7. Consistent naming and structure across all protocols\n8. Custom exception types used for error handling\n9. Rich return types with detailed metadata\n10. Support for configuration options via **kwargs\n11. Integration with domain models (CoverageResult, RefineOutcome, AnalysisReport)\n12. No linting errors detected",
        "testStrategy": "Test implementations for each protocol verify interface compliance. mypy is used to ensure type checking works correctly with all protocols. Linting and static analysis confirm code quality and adherence to standards.",
        "subtasks": [
          {
            "id": 1,
            "title": "Verify protocol files and exports",
            "description": "Check that all 12 protocol files exist in testcraft/ports/ and that __init__.py exports each interface.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Run mypy and linting on ports directory",
            "description": "Ensure all protocol files pass mypy type checking and linting with no errors.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create test implementations for each protocol",
            "description": "Write minimal test classes for each protocol to verify interface compliance and demonstrate usage.",
            "status": "done",
            "dependencies": [],
            "details": "",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Parser Adapters",
        "description": "Create adapters for parsing Python code and mapping tests to source code elements. The parser must extract both structural metadata and the actual source code content for each code element to support test generation.",
        "status": "done",
        "dependencies": [
          4
        ],
        "priority": "high",
        "details": "1. Create adapters/parsing/codebase_parser.py:\n   - Implement AST-based parsing of Python files\n   - Extract functions, classes, methods with signatures\n   - Parse docstrings and type annotations\n   - Identify import statements and dependencies\n   - Extract the raw source code content for each code element (function, class, method) in addition to metadata\n   - Ensure parsing results include both metadata (name, type, line_range, docstring) and the actual implementation code\n   - Consider whether the TestElement domain model should be extended with a 'source_code' field, or if this content should be handled separately in the parsing pipeline\n2. Create adapters/parsing/test_mapper.py:\n   - Map test functions to source code elements\n   - Identify existing test coverage for elements\n   - Support pytest naming conventions\n   - Ensure mapping logic can access both metadata and source code content for each element\n3. Implement helper functions for:\n   - Building directory trees with bounded depth/width\n   - Extracting element signatures and source code\n   - Identifying uncovered elements\n   - Caching parsed files to improve performance, including both metadata and source code content",
        "testStrategy": "Unit tests with sample Python files to verify correct parsing of functions, classes, methods, and imports. Test that both metadata and actual source code content are extracted for each code element. Test mapping of test functions to source elements with various naming conventions, ensuring access to source code content. Verify directory tree generation with different depth/width constraints.",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Coverage Adapters",
        "description": "All coverage adapters for measuring code coverage using pytest+coverage with AST fallback are now implemented and production-ready. The solution provides robust coverage measurement, fallback capabilities, and comprehensive reporting.\n\nKey components:\n- adapters/coverage/pytest_coverage.py: Full pytest+coverage integration with command building, environment setup, coverage parsing (JSON/XML), timeout handling, and artifact storage in .testcraft/coverage/<run_id>\n- adapters/coverage/ast_fallback.py: AST-based coverage estimation with executable line detection, complex construct analysis, private function filtering, and realistic coverage estimation\n- adapters/coverage/composite.py: Composite adapter that tries pytest first and falls back to AST with reason tracking and unified interface\n- adapters/coverage/main_adapter.py: Primary adapter implementing CoveragePort interface with all required methods, HTML/JSON report generation, and comprehensive coverage analysis\n- adapters/coverage/__init__.py: Exports all adapters\n- test_coverage_adapters.py: Comprehensive test suite with unit and integration tests for all adapters\n\nAll adapters handle error cases, timeouts, and edge cases. Code is lint-free and follows project standards.",
        "status": "done",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "details": "Coverage adapters are implemented as follows:\n\n1. adapters/coverage/pytest_coverage.py:\n   - Builds and runs pytest+coverage commands with correct cwd, env, and pythonpath\n   - Parses coverage results from JSON and XML outputs\n   - Handles timeouts, subprocess errors, and stores artifacts in .testcraft/coverage/<run_id>\n2. adapters/coverage/ast_fallback.py:\n   - Estimates line and branch coverage using AST analysis\n   - Detects executable lines, analyzes complex constructs, filters private functions\n   - Provides realistic fallback coverage estimation when pytest coverage fails\n3. adapters/coverage/composite.py:\n   - Attempts pytest coverage first, falls back to AST estimation on failure\n   - Records reason for fallback and exposes a unified interface\n4. adapters/coverage/main_adapter.py:\n   - Implements CoveragePort interface\n   - Provides HTML and JSON report generation\n   - Aggregates and analyzes coverage results from all adapters\n5. __init__.py exports all adapters for external use\n6. Comprehensive test suite (test_coverage_adapters.py):\n   - Unit tests for each adapter\n   - Integration tests for composite behavior and error handling\n   - Tests for edge cases, timeouts, and artifact correctness\n\nAll code is linted and adheres to project standards.",
        "testStrategy": "Comprehensive test suite with unit tests for all adapters and integration tests for composite behavior. Tests verify coverage measurement with pytest, AST fallback with various Python constructs, correct fallback logic, error and timeout handling, and artifact storage. All adapters are validated against edge cases and project linting standards.",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Implement Writer Adapters",
        "description": "Create adapters for safely writing generated tests to the filesystem with different strategies. All writes must be formatted with Black and isort for consistent code formatting.",
        "status": "done",
        "dependencies": [
          4,
          5
        ],
        "priority": "high",
        "details": "1. Create adapters/io/writer_append.py:\n   - Implement simple append to existing test files\n   - Create new files when missing\n   - Support dry-run mode\n   - Format all writes with Black and isort\n2. Create adapters/io/writer_ast_merge.py:\n   - Parse existing and new test files\n   - Merge structurally to avoid duplicates\n   - Format all writes with Black and isort\n   - Generate unified diff for dry-run\n3. Implement safety policies:\n   - Only write to tests/ directory\n   - Enforce file size caps\n   - Block dangerous patterns\n   - Validate syntax before writing\n4. Add helper functions for path resolution and validation\n5. Integrate Black and isort formatting into all write operations, including dry-run output.",
        "testStrategy": "Unit tests for append and AST merge strategies with various test files. Verify safety policies block writes outside tests/ directory and files with dangerous patterns. Test dry-run mode generates correct diffs without modifying files. Validate that all written and diffed files are formatted with Black and isort.",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate Black and isort formatting into writer_append.py",
            "description": "Update writer_append.py so that all writes (including dry-run output) are formatted using Black and isort before being written or diffed.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:03:58.368Z>\n✅ COMPLETED - Black and isort formatting integration in writer_append.py\n\nImplementation Details:\n- Added _format_content() method that uses subprocess to call both isort and Black\n- Format pipeline: content → isort (import sorting) → Black (code formatting) → formatted output\n- Graceful fallback: returns original content if formatting tools aren't available\n- All write operations automatically format content before writing or showing diffs\n- Dry-run mode also shows formatted content in previews\n\nKey Features:\n- Temporary file approach for safe formatting without affecting original content\n- Error handling prevents failures when Black/isort not installed\n- Consistent formatting applied to both new files and appended content\n- Integration verified through unit tests with subprocess mocking\n</info added on 2025-09-07T15:03:58.368Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integrate Black and isort formatting into writer_ast_merge.py",
            "description": "Update writer_ast_merge.py so that all merged output (including dry-run diff) is formatted using Black and isort before being written or diffed.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:04:17.585Z>\n✅ COMPLETED - Black and isort formatting integration in writer_ast_merge.py\n\nImplementation Details:\n- Added a _format_content() method, mirroring the approach used in writer_append.py, to ensure consistent formatting logic across writer adapters.\n- The formatting pipeline applies isort for import sorting followed by Black for code formatting to all merged content before output, guaranteeing standardized and readable code.\n- Both the final merged output and the dry-run diff generation now utilize this formatting pipeline, ensuring that diffs accurately reflect the formatted result.\n- The implementation includes graceful fallback handling: if Black or isort are unavailable, the merge proceeds without formatting rather than failing.\n\nAST Merge Features Enhanced:\n- The AST merging logic now preserves the existing code structure while intelligently adding new elements, minimizing unnecessary changes.\n- Enhanced deduplication ensures that imports, functions, and classes are merged without introducing duplicates.\n- The merge process is structurally aware: imports are placed first, followed by constants, classes, functions, and then other statements, resulting in a logical and maintainable code order.\n- If AST parsing fails, the system falls back to simple concatenation to avoid blocking the merge process.\n- Dry-run mode now generates a unified diff that precisely shows the changes that would be made after formatting, improving transparency and reviewability.\n</info added on 2025-09-07T15:04:17.585Z>",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Update unit tests to validate Black and isort formatting",
            "description": "Extend unit tests to check that all written and diffed files are correctly formatted with Black and isort.",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-09-07T15:04:22.973Z>\nComprehensive unit tests have been implemented to validate all core writer adapter behaviors, including strict enforcement of Black and isort formatting on written and diffed files. Tests cover SafetyPolicies (path validation, size limits, dangerous pattern detection, Python syntax validation, test file naming), WriterAppendAdapter (file creation, append, dry-run, backup, directory creation, formatting integration), and WriterASTMergeAdapter (AST merge logic, deduplication, diff generation, merge fallbacks). Key features include mocked subprocess calls for Black/isort, fallback behavior when formatting fails, exhaustive safety policy scenarios, dry-run mode validation, complex AST merge cases, and robust error/exception handling.\n</info added on 2025-09-07T15:04:22.973Z>",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Implement State Management",
        "description": "Create adapters for managing state across runs, including coverage history and generation logs.",
        "details": "1. Create adapters/io/state_json.py:\n   - Implement project-scoped JSON state storage\n   - Optional SQLite backend support\n   - Maintain coverage history\n   - Track generation log\n   - Support idempotent decisions\n2. Implement methods for:\n   - Initializing state\n   - Updating state after generation\n   - Querying historical data\n   - Determining which files need generation\n3. Add migration support from old .testgen_state.json format\n4. Implement state synchronization and reset commands",
        "testStrategy": "Unit tests for state initialization, updates, and queries. Test migration from old state format. Verify idempotent decisions work correctly across multiple runs. Test state reset and synchronization.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Implement Telemetry and Cost Management",
        "description": "Create adapters for OpenTelemetry tracing, metrics collection, and cost management.",
        "details": "1. Create adapters/telemetry/otel.py:\n   - Implement OpenTelemetry spans for use cases\n   - Track LLM calls, retrieval, coverage runs, writer operations\n   - Record metrics for latency, errors, tokens\n   - Support opt-out and anonymization\n2. Create adapters/telemetry/metrics.py:\n   - Track coverage delta\n   - Count tests generated\n   - Measure pass rate\n   - Count refine iterations\n3. Create adapters/telemetry/cost_manager.py:\n   - Track token usage per request\n   - Accrue costs by model and operation\n   - Enforce budget warnings and stops\n   - Persist summary by day\n   - Implement cost optimization strategies",
        "testStrategy": "Unit tests for span creation and attribute recording. Test metrics collection and aggregation. Verify cost tracking across multiple requests and budget enforcement. Test opt-out functionality and anonymization.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Implement Reporting and Artifact Storage",
        "description": "Create adapters for generating reports and storing artifacts from test generation runs.",
        "details": "1. Create adapters/io/reporter_json.py:\n   - Generate structured JSON reports\n   - Include coverage delta, tests generated, pass rate\n   - Record prompts and schemas (when verbose)\n   - Summarize retrieval diagnostics\n2. Create adapters/io/artifact_store.py:\n   - Store coverage reports\n   - Save generated tests\n   - Preserve LLM responses\n   - Manage run history\n   - Implement cleanup policies\n3. Implement helper functions for:\n   - Formatting tables for CLI output\n   - Generating spinners and progress indicators\n   - Creating concise summaries",
        "testStrategy": "Unit tests for report generation with various inputs. Test artifact storage and retrieval. Verify cleanup policies work correctly. Test formatting functions for CLI output.",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Implement Prompt Registry and Templates",
        "description": "Create a registry for versioned prompt templates with system and user prompts for generation and refinement.",
        "details": "1. Create prompts/registry.py:\n   - Implement versioned templates\n   - Store system prompts for generation and refinement\n   - Store user prompt templates\n   - Define JSON schemas for structured outputs\n2. Create generation system prompt with:\n   - Role: \"Python Test Generation Agent\"\n   - Constraints on output format and safety\n   - Guardrails against modifying source files\n3. Create refinement system prompt with:\n   - Role: \"Python Test Refiner\"\n   - Constraints on fixing specific issues\n4. Implement JSON schemas for:\n   - Generation output (file path and content)\n   - Refinement output (updated files, rationale, plan)\n5. Add anti-injection defenses in prompts",
        "testStrategy": "Unit tests for prompt template rendering with various inputs. Verify JSON schemas validate correct outputs and reject invalid ones. Test anti-injection defenses with potentially problematic inputs.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Implement Context Pipeline",
        "description": "Create adapters for indexing, retrieving, and summarizing code context from the repository.",
        "details": "1. Create context/indexer.py:\n   - Implement hybrid index (BM25 + dense)\n   - Store chunk metadata (path, symbol, imports)\n   - Support adaptive chunking (semantic boundaries, AST nodes)\n   - Typical chunk size 300-800 tokens\n2. Create context/retriever.py:\n   - Build queries from filename, symbols, docstrings\n   - Include uncovered lines context\n   - Support optional recent git diffs\n   - Implement HyDE expansion and query rewriting\n   - Rerank with cross-encoder\n   - Select top-k snippets\n3. Create context/summarizer.py:\n   - Generate directory tree with bounded breadth/depth\n   - Summarize imports and class signatures\n   - Enforce max chars for user prompt content\n4. Implement context budgeting and truncation strategies",
        "testStrategy": "Unit tests for indexing with sample repositories. Test retrieval with various queries and verify relevant snippets are returned. Test summarization with different directory structures. Verify context budgeting works correctly with large repositories.",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement LLM Adapters",
        "description": "Create adapters for different LLM providers with common helpers for response parsing, validation, and error handling.",
        "details": "1. Create llm/common.py:\n   - Implement response parsing with brace balancing\n   - Add strict JSON Schema validation\n   - Support repair attempts for minor issues\n   - Normalize output (strip code fences, unescape)\n   - Log cost and token usage\n   - Implement retries with jitter\n   - Respect rate limits\n   - Set timeouts\n2. Create provider-specific adapters:\n   - llm/claude.py for Anthropic Claude\n   - llm/openai.py for OpenAI models\n   - llm/azure.py for Azure OpenAI\n   - llm/bedrock.py for AWS Bedrock\n3. Implement model routing based on file complexity\n4. Use deterministic settings for structure (temperature 0.2-0.3 for generation; lower for refine)\n5. Support streaming responses for large outputs",
        "testStrategy": "Unit tests for each provider adapter with mocked responses. Test response parsing with various formats including malformed JSON. Verify retry logic works with different error types. Test model routing with files of varying complexity.",
        "priority": "high",
        "dependencies": [
          4,
          11
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Refinement System",
        "description": "Create adapters for refining generated tests based on pytest failures and other issues.",
        "details": "1. Create refine/manager.py:\n   - Categorize failures (import errors, assertion mismatches, fixture/mocks)\n   - Select appropriate refinement strategy\n   - Cap iterations and total time\n   - Stop on no-change\n   - Safely apply updates\n   - Rerun pytest to verify fixes\n2. Implement refinement strategies:\n   - refine/strategies/import_fix.py\n   - refine/strategies/assertion_fix.py\n   - refine/strategies/fixture_fix.py\n3. Create payload builder in application layer:\n   - Include failures, artifacts, repo context\n   - Add last run command, tests written, git metadata\n   - Analyze and include pattern information\n4. Implement safe apply closure and rerun closure",
        "testStrategy": "Unit tests for failure categorization with sample pytest outputs. Test each refinement strategy with specific failure types. Verify iteration caps and time limits work correctly. Test safe apply with various test files.",
        "priority": "medium",
        "dependencies": [
          6,
          7,
          13
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Implement Analyze Use Case",
        "description": "Create the application layer use case for analyzing what would be generated and why.",
        "details": "1. Create application/analyze_usecase.py:\n   - Implement steps: sync state → find files → coverage.measure\n   - For each file call state.should_generate\n   - Build plans (elements from parser; reasons; existing test presence)\n   - Produce AnalysisReport DTO\n2. Use dependency injection for ports\n3. Keep business logic pure (no direct adapter calls)\n4. Implement helper functions for:\n   - Determining which elements need tests\n   - Calculating coverage gaps\n   - Prioritizing files for generation",
        "testStrategy": "Unit tests with mocked ports to verify correct analysis flow. Test with various repository states and coverage levels. Verify correct identification of elements needing tests and reasons for generation.",
        "priority": "medium",
        "dependencies": [
          4,
          5,
          6,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Implement Coverage Use Case",
        "description": "Create the application layer use case for measuring and reporting code coverage.",
        "details": "1. Create application/coverage_usecase.py:\n   - Implement steps: sync state → find files → coverage.measure → coverage.report\n   - Support filtering by file patterns\n   - Calculate overall and per-file coverage\n   - Generate coverage reports\n2. Use dependency injection for ports\n3. Keep business logic pure (no direct adapter calls)\n4. Implement helper functions for:\n   - Formatting coverage results\n   - Identifying coverage trends over time\n   - Suggesting files for improvement",
        "testStrategy": "Unit tests with mocked ports to verify correct coverage measurement and reporting. Test with various repository states and coverage levels. Verify correct calculation of overall and per-file coverage.",
        "priority": "medium",
        "dependencies": [
          4,
          6,
          8
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement Generate Use Case",
        "description": "Create the application layer use case for generating tests, the core functionality of the system.",
        "details": "1. Create application/generate_usecase.py with the following steps:\n   - Sync state; discover files; coverage.measure\n   - Decide files to process; build TestGenerationPlan per file\n   - Build directory tree; gather codebase info; retrieve context if enabled\n   - Apply batching policy (streaming or batch N)\n   - Build prompts with system and user content\n   - Call LLMPort.generate_tests with JSON Schema; validate; normalize\n   - Use WriterPort.write per file with configured strategy\n   - Optionally run pytest via CoveragePort; parse failures\n   - Optionally refine via RefinePort based on failure category\n   - Measure coverage delta; record state; report; telemetry + cost summary\n2. Use dependency injection for all ports\n3. Keep business logic pure (no direct adapter calls)\n4. Implement batching and concurrency strategies\n5. Add proper error handling and recovery",
        "testStrategy": "Unit tests with mocked ports to verify correct generation flow. Test with various repository states, coverage levels, and configuration options. Verify correct handling of LLM responses, writing strategies, and refinement. Test error handling and recovery scenarios.",
        "priority": "high",
        "dependencies": [
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Implement Status and Utility Use Cases",
        "description": "Create application layer use cases for viewing generation state/history and utility functions.",
        "details": "1. Create application/status_usecase.py:\n   - Implement view generation state/history\n   - Support filtering and sorting options\n   - Generate summary statistics\n2. Create application/utility_usecases.py for:\n   - debug-state: dump internal state for debugging\n   - sync-state: force state synchronization\n   - reset-state: clear state and start fresh\n   - env: show environment information\n   - cost: display cost summary and projections\n3. Use dependency injection for all ports\n4. Keep business logic pure (no direct adapter calls)",
        "testStrategy": "Unit tests with mocked ports to verify correct status reporting and utility functions. Test with various repository states and history records. Verify correct handling of state operations and environment information.",
        "priority": "low",
        "dependencies": [
          4,
          8,
          9
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Implement CLI Interface",
        "description": "Create the command-line interface for all commands with proper argument parsing and output formatting.",
        "details": "1. Create cli/main.py:\n   - Implement commands: generate, analyze, coverage, status\n   - Add utility commands: init-config, env, cost, debug-state, sync-state, reset-state\n   - Support flags: batch-size, streaming, force, dry-run, model options, etc.\n   - Format output with concise tables, spinners, summaries\n   - Add verbose mode for detailed diagnostics\n2. Create cli/config_init.py:\n   - Generate full YAML with commented guidance\n   - Provide minimal preset option\n3. Implement dependency injection for use cases\n4. Add proper error handling and user-friendly messages\n5. Support plugin discovery via entry points",
        "testStrategy": "Unit tests for command-line argument parsing and validation. Test output formatting with various results. Verify error handling provides useful messages. Test config initialization with different options.",
        "priority": "medium",
        "dependencies": [
          3,
          15,
          16,
          17,
          18
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Implement Evaluation Harness and Documentation",
        "description": "Create an evaluation system for testing the quality of generated tests and comprehensive documentation.",
        "details": "1. Create evaluation/harness.py:\n   - Implement offline golden repos testing\n   - Add acceptance checks: syntactic validity, importability, pytest success, coverage improvements\n   - Support LLM-as-judge with rubric for test quality (optional)\n   - Enable SxS prompt variants A/B testing\n   - Store prompt registry version in artifacts\n2. Create comprehensive documentation:\n   - README.md with quickstart guide\n   - Advanced usage documentation\n   - Configuration reference\n   - Architecture overview\n   - Contributing guidelines\n3. Create sample .testgen.yml files:\n   - Minimal configuration\n   - Comprehensive configuration with comments\n4. Set up CI pipeline:\n   - Lint with ruff\n   - Type-check with mypy\n   - Run tests with pytest\n   - Verify documentation builds",
        "testStrategy": "Test the evaluation harness with sample repositories. Verify documentation is accurate and comprehensive. Test CI pipeline with various code changes to ensure it catches issues.",
        "priority": "low",
        "dependencies": [
          17,
          19
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Implement Rich-based UI System for CLI",
        "description": "Create a comprehensive UI system using the Rich library to provide beautiful, interactive command-line interfaces with progress bars, tables, panels, syntax highlighting, and interactive components.",
        "details": "1. Create adapters/ui/rich_adapter.py implementing UIPort:\n   - Implement the UIPort interface defined in Task 4\n   - Create a RichAdapter class with methods for all required UI operations\n   - Configure Rich theme settings with consistent color schemes\n   - Add support for terminal width detection and responsive layouts\n   - Implement graceful fallbacks for terminals without Rich support\n\n2. Implement Rich UI Components:\n   - Create ui/components/ directory with reusable Rich components:\n     - TestResultsTable: Display test generation results with syntax highlighting\n     - CoverageReportPanel: Show coverage metrics with color-coded indicators\n     - ProgressTracker: Display progress for multi-step operations\n     - ErrorDisplay: Format errors with traceback highlighting\n     - ConfigurationWizard: Interactive setup with form inputs\n   - Ensure components are themeable and configurable\n\n3. Implement Progress Visualization:\n   - Create display_progress() with Rich progress bars\n   - Support for nested progress tracking (e.g., file-level and test-level)\n   - Add spinners for indeterminate operations\n   - Implement ETA calculations for long-running tasks\n   - Support for cancellation and pause/resume indicators\n\n4. Implement Results Display:\n   - Create display_results() with Rich tables and panels\n   - Format test results with syntax highlighting for code snippets\n   - Color-code success/failure states\n   - Support for collapsible sections for detailed information\n   - Add summary statistics at the top of reports\n\n5. Add Integration Features:\n   - Support both verbose and quiet output modes\n   - Implement proper terminal capabilities detection\n   - Create plain text fallback renderer for non-interactive environments\n   - Integrate with the existing logging system\n   - Support output redirection and piping\n   - Add configuration options for UI preferences",
        "testStrategy": "1. Unit Tests:\n   - Test RichAdapter implementation against the UIPort interface\n   - Verify each UI component renders correctly with different inputs\n   - Test responsive layout with various terminal widths\n   - Verify fallback mechanisms work when Rich features are unavailable\n   - Test integration with the logging system\n\n2. Integration Tests:\n   - Create mock test generation scenarios and verify UI components display correctly\n   - Test progress tracking with simulated long-running operations\n   - Verify error display with various error types\n   - Test interactive components with simulated user input\n\n3. Visual Verification:\n   - Create a test script that demonstrates all UI components\n   - Capture screenshots of UI components for documentation\n   - Verify color schemes are consistent across components\n   - Test with different terminal types and color schemes\n\n4. Edge Cases:\n   - Test with extremely large datasets to verify table pagination\n   - Verify behavior when terminal is resized during operation\n   - Test with redirected output and in CI environments\n   - Verify accessibility considerations (color contrast, etc.)",
        "status": "pending",
        "dependencies": [
          4,
          19
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-01-27T20:16:58.061Z",
      "updated": "2025-09-07T21:18:12.840Z",
      "description": "Tasks for master context"
    }
  }
}